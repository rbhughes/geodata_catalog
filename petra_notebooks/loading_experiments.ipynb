{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec549db-e04d-4ddb-a8de-89ddee1b9cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We're ingesting files from https://github.com/rbhughes/purr_petra, which generates well-centric json extracted from DBISAM on Windows. Some datatypes are relatively flat; others will need quite a bit of processing to get them into a reasonable tabular format. We'll also generate a unique id based on the source project/repo + datatype + whatever else.\n",
    "\n",
    "databricks cli can't do wildcards on cp, so loop to copy into the volume instead:\n",
    "\n",
    "`bryan@ichabod mac_bucket % for file in *_well.json; do\n",
    "  databricks fs cp \"$file\" dbfs:/Volumes/geodata/petra/well/\n",
    "done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2863d6e5-4287-4945-8f3b-1ad2e91d669a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"/Volumes/geodata/petra/well/\")\n",
    "\n",
    "# df.printSchema()\n",
    "# df.dtypes\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29054ac1-5f33-44e0-bc0f-e5482fabe8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # so, the notebook just crapped out and stopped printing results. Re-attach worked, these did not:\n",
    "# import common.transforms\n",
    "# import sys\n",
    "# print(dir(common.transforms))\n",
    "# sys.stdout.flush()\n",
    "# print('hello?')\n",
    "# sys.stdout.write('hello\\n')\n",
    "# sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f416644-c727-40fa-8518-2c8c037e46a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from common.transforms import string_to_iso_date, generate_hash\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_flat = df.select(\n",
    "    F.col(\"repo_id\"),\n",
    "    F.col(\"uwi.uwi\").alias(\"uwi\"),\n",
    "    F.col(\"uwi.wsn\").alias(\"wsn\"),\n",
    "    F.col(\"locat.lat\").alias(\"surface_latitude\"),\n",
    "    F.col(\"locat.lon\").alias(\"surface_longitude\"),\n",
    "    F.col(\"bhloc.lat\").alias(\"bottom_latitude\"),\n",
    "    F.col(\"bhloc.lon\").alias(\"bottom_longitude\"),\n",
    "    F.col(\"well.county\").alias(\"county\"),\n",
    "    F.col(\"well.state\").alias(\"state\"),\n",
    "    F.col(\"well.fieldname\").alias(\"field_name\"),\n",
    "    F.col(\"well.histoper\").alias(\"historical_operator\"),\n",
    "    F.col(\"well.label\").alias(\"well_label\"),\n",
    "    F.col(\"well.leasename\").alias(\"lease_name\"),\n",
    "    F.col(\"well.leasenumber\").alias(\"lease_number\"),\n",
    "    F.col(\"well.operator\").alias(\"operator\"),\n",
    "    F.col(\"well.prodfm\").alias(\"producing_formation\"),\n",
    "    F.col(\"well.remarks\").alias(\"remarks\"),\n",
    "    F.col(\"well.shortname\").alias(\"short_name\"),\n",
    "    F.col(\"well.wellname\").alias(\"well_name\"),\n",
    "    F.col(\"well.symcode\").alias(\"symbol\"),\n",
    "    F.col(\"zdata.aband_date\").alias(\"abandonment_date\"),\n",
    "    F.col(\"zflddef.active_datum\").alias(\"active_datum\"),\n",
    "    F.col(\"zdata.active_datum_value\").alias(\"active_datum_value\"),\n",
    "    F.col(\"zdata.comp_date\").alias(\"completion_date\"),\n",
    "    F.col(\"zdata.cumgas\").alias(\"cum_gas\"),\n",
    "    F.col(\"zdata.cumoil\").alias(\"cum_oil\"),\n",
    "    F.col(\"zdata.cumwtr\").alias(\"cum_water\"),\n",
    "    F.col(\"zdata.elev_df\").alias(\"elev_df\"),\n",
    "    F.col(\"zdata.elev_gr\").alias(\"elev_gr\"),\n",
    "    F.col(\"zdata.elev_kb\").alias(\"elev_kb\"),\n",
    "    F.col(\"zdata.last_act_date\").alias(\"last_activity_date\"),\n",
    "    F.col(\"zdata.permit_date\").alias(\"permit_date\"),\n",
    "    F.col(\"zdata.rig_date\").alias(\"rig_date\"),\n",
    "    F.col(\"zdata.report_date\").alias(\"report_date\"),\n",
    "    F.col(\"zdata.spud_date\").alias(\"spud_date\"),\n",
    "    F.col(\"zdata.td\").alias('total_depth'),\n",
    "    F.col(\"zdata.whipstock\").alias(\"whipstock\"),\n",
    "    F.col(\"zdata.wtrdepth\").alias(\"water_depth\"),\n",
    "    F.col(\"well.adddate\").alias(\"app_row_created\"),\n",
    "    F.col(\"well.chgdate\").alias(\"app_row_changed\")\n",
    ")\n",
    "\n",
    "date_columns = [\n",
    "    \"abandonment_date\", \n",
    "    \"completion_date\", \n",
    "    \"last_activity_date\", \n",
    "    \"permit_date\", \n",
    "    \"report_date\", \n",
    "    \"spud_date\",\n",
    "    \"rig_date\",\n",
    "    \"app_row_created\",\n",
    "    \"app_row_changed\"\n",
    "]\n",
    "\n",
    "df_well = df_flat\n",
    "\n",
    "\n",
    "# enforce timestamp for dates\n",
    "for col_name in date_columns:\n",
    "    df_well = string_to_iso_date(df_well, col_name, col_name)\n",
    "\n",
    "# add id hash\n",
    "id_columns = [\"repo_id\", \"uwi\"]\n",
    "df_well = generate_hash(df_well, \"id\", \"well\", *id_columns)\n",
    "\n",
    "\n",
    "display(df_well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6271e6-a847-426c-8226-6031e2e3fc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },

   "source": [
    "commit test"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6359062826259343,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "loading_experiments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
