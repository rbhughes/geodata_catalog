{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f010193-fb12-4acf-a2c6-76f0d01d4ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec549db-e04d-4ddb-a8de-89ddee1b9cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We're ingesting files from https://github.com/rbhughes/purr_petra, which generates well-centric json extracted from DBISAM on Windows. Some datatypes are relatively flat; others will need quite a bit of processing to get them into a reasonable tabular format. We'll also generate a unique id based on the source project/repo + datatype + whatever else.\n",
    "\n",
    "databricks cli can't do wildcards on cp, so loop to copy into the volume instead:\n",
    "\n",
    "`bryan@ichabod mac_bucket % for file in *_formation.json; do\n",
    "  databricks fs cp \"$file\" dbfs:/Volumes/geodata/petra/formation/\n",
    "done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2863d6e5-4287-4945-8f3b-1ad2e91d669a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"/Volumes/geodata/petra/well/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560cc15d-171a-470e-a494-765341dc7ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.printSchema()\n",
    "# df.dtypes\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86651622-b322-4e86-959f-afd097f64fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import to_timestamp, col\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def convert_iso_to_timestamp(df: DataFrame, string_col: str, new_col: str) -> DataFrame:\n",
    "#     return df.withColumn(new_col, to_timestamp(col(string_col), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def convert_iso_to_timestamp(df: DataFrame, string_col: str, new_col: str) -> DataFrame:\n",
    "    return df.withColumn(new_col, F.to_timestamp(F.col(string_col), \"yyyy-MM-dd'T'HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f416644-c727-40fa-8518-2c8c037e46a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_with_coords = df.withColumn(\"locat_lat\", df[\"locat.lat\"])\\\n",
    "#    .withColumn(\"locat_lon\", df[\"locat.lon\"])\\\n",
    "#    .drop(\"locat\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df_a = df.select(\"*\",\n",
    "    df.uwi.uwi.alias(\"uwi\"),\n",
    "    df.uwi.wsn.alias(\"wsn\"),\n",
    "    df.locat.lat.alias(\"surface_latitude\"),\n",
    "    df.locat.lon.alias(\"surface_longitude\"),\n",
    "    df.bhloc.lat.alias(\"bottom_latitude\"),\n",
    "    df.bhloc.lon.alias(\"bottom_longitude\"),\n",
    "    df.well.county.alias(\"county\"),\n",
    "    df.well.state.alias(\"state\"),\n",
    "    df.well.fieldname.alias(\"field_name\"),\n",
    "    df.well.histoper.alias(\"historical_operator\"),\n",
    "    df.well.label.alias(\"well_label\"),\n",
    "    df.well.leasename.alias(\"lease_name\"),\n",
    "    df.well.leasenumber.alias(\"lease_number\"),\n",
    "    df.well.operator.alias(\"operator\"),\n",
    "    df.well.prodfm.alias(\"producing_formation\"),\n",
    "    df.well.remarks.alias(\"remarks\"),\n",
    "    df.well.shortname.alias(\"short_name\"),\n",
    "    df.well.wellname.alias(\"well_name\"),\n",
    "    df.well.symcode.alias(\"symbol\"),\n",
    "    df.zdata.aband_date.alias(\"abandonment_date\"),\n",
    "    df.zflddef.active_datum.alias(\"active_datum\"),\n",
    "    df.zdata.active_datum_value.alias(\"active_datum_value\"),\n",
    "    df.zdata.comp_date.alias(\"completion_date\"),\n",
    "    df.zdata.cumgas.alias(\"cum_gas\"),\n",
    "    df.zdata.cumoil.alias(\"cum_oil\"),\n",
    "    df.zdata.cumwtr.alias(\"cum_water\"),\n",
    "    df.zdata.elev_df.alias(\"elev_df\"),\n",
    "    df.zdata.elev_gr.alias(\"elev_gr\"),\n",
    "    df.zdata.elev_kb.alias(\"elev_kb\"),\n",
    "    df.zdata.last_act_date.alias(\"last_activity_date\"),\n",
    "    df.zdata.permit_date.alias(\"permit_date\"),\n",
    "    df.zdata.rig_date.alias(\"rig_date\"),\n",
    "    df.zdata.report_date.alias(\"report_date\"),\n",
    "    df.zdata.spud_date.alias(\"spud_date\"),\n",
    "    df.zdata.td.alias('total_depth'),\n",
    "    df.zdata.whipstock.alias(\"whipstock\"),\n",
    "    df.zdata.wtrdepth.alias(\"water_depth\"),\n",
    "    df.well.adddate.alias(\"app_row_created\"),\n",
    "    df.well.chgdate.alias(\"app_row_changed\")\n",
    "\n",
    ").drop(\"locat\", \"bhloc\", \"uwi\", \"well\", \"zdata\", \"zflddef\")\n",
    "\n",
    "display(df_a)\n",
    "\n",
    "date_columns = [\n",
    "    \"abandonment_date\", \n",
    "    \"completion_date\", \n",
    "    \"last_activity_date\", \n",
    "    \"permit_date\", \n",
    "    \"report_date\", \n",
    "    \"spud_date\",\n",
    "    \"app_row_created\",\n",
    "    \"app_row_changed\"\n",
    "]\n",
    "\n",
    "df_b = df_a\n",
    "for col_name in date_columns:\n",
    "    df_b = convert_iso_to_timestamp(df_b, col_name, col_name)\n",
    "\n",
    "display(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef6271e6-a847-426c-8226-6031e2e3fc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "loading_experiments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
